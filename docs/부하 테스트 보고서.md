# 콘서트 예약 서비스 부하 테스트 보고서

**작성일**: 2025년 12월 8일  
**작성자**: 문예진  
**테스트 환경**: Docker Desktop (Apple Silicon)

---

## 1. 개요

### 1.1 배경
콘서트 예약 서비스는 티켓 오픈 시 순간적인 트래픽 집중이 발생합니다. 특히 결제 처리는 금융 시스템 수준의 신뢰성이 요구되며, 다음 항목들이 보장되어야 합니다:
- 중복 결제 방지
- 잔액 정합성 100% 보장
- 동시 요청 순서 보장

### 1.2 목적
1. 시스템 최대 처리량(TPS) 및 한계점 파악
2. 리소스별 적정 배포 스펙 도출
3. 병목 구간 식별 및 개선 방향 수립
4. SLA 기준 수립을 위한 기초 데이터 확보
5. **동시성 제어 검증** - 좌석 중복 예약 방지 확인

### 1.3 테스트 대상
| API | 설명 | 선정 이유 |
|-----|------|----------|
| GET /api/wallet/{userId}/balance | 잔액 조회 | 가장 빈번한 호출, 캐시 효과 측정 |
| POST /api/queue/token | 대기열 토큰 발급 | 트래픽 제어 진입점 |
| POST /api/reservations/temporary-assign | 좌석 임시 배정 | 핵심 동시성 경쟁 구간 |

---

## 2. 테스트 환경

### 2.1 인프라 구성
| 컴포넌트 | 버전 | 비고 |
|---------|------|------|
| Spring Boot | 3.x | 애플리케이션 서버 |
| MySQL | 8.0 | 데이터베이스 |
| Redis | 7.2 | 캐시, 대기열, 분산 락 |
| Kafka | 7.5.0 | 이벤트 스트리밍 (3-broker 클러스터) |

### 2.2 테스트 도구
- **JMeter 5.6.3**

### 2.3 테스트 데이터
- **테스트 유저: 10,000명** (test-user-00001 ~ test-user-10000)
- 초기 잔액: 1,000,000원
- 콘서트: 3개
- 스케줄: 9개 (콘서트당 3일)
- 좌석: 스케줄당 50석
- UUID 패턴: `00000000-0000-0000-0000-XXXXXXXXXXXX`

---

## 3. 테스트 시나리오

### 3.1 잔액 조회 부하 테스트
| 항목 | 설정값 |
|------|--------|
| 동시 사용자 (Threads) | 100 |
| Ramp-up 시간 | 30초 |
| 테스트 지속 시간 | 60초 |
| 대상 API | GET /api/wallet/{userId}/balance |

### 3.2 예약 플로우 부하 테스트
| 항목 | 설정값 |
|------|--------|
| 동시 사용자 (Threads) | 50 / 100 / 500 |
| Ramp-up 시간 | 10~30초 |
| Loop Count | 1 (1인당 1회 시도) |
| 대상 좌석 | 50석 (스케줄 ID: 1) |
| 유저 풀 | 10,000명 |

**테스트 플로우:**
1. POST /api/queue/token - 대기열 토큰 발급
2. POST /api/reservations/temporary-assign - 좌석 임시 배정 (토큰 필요)

---

## 4. 테스트 결과

### 4.1 잔액 조회 - 리소스별 성능 비교

| 스펙 | TPS | 평균(ms) | 최대(ms) | 에러율 | CPU% | MEM% | 상태        |
|------|-----|---------|---------|--------|------|------|-----------|
| 0.25CPU / 256M | - | - | - | 100% | 25% | 99.9% | 🔴 메모리 부족 |
| 0.5CPU / 512M | 134 | 560 | 5,699 | 0% | - | - | 🟡 느림     |
| 1CPU / 512M | 4,088 | 18 | 7,846 | 0% | - | - | 🟢 양호     |
| 2CPU / 1G | 3,380 | 22 | 7,888 | 0% | 208% | 39% | 🟢 양호     |

### 4.2 예약 플로우 - 동시성 경쟁 테스트 (1CPU / 512M)

| 시나리오 | 동시 사용자 | 총 요청 | TPS | 평균(ms) | 좌석 경쟁 실패율 | 예약 성공 | 중복 예약 |
|---------|-----------|--------|-----|---------|----|----------|----------|
| 50명 vs 50좌석 | 50 | 100 | 10/s | 14 | 50% | 50건 | 0건 ✅ |
| 100명 vs 50좌석 | 100 | 200 | 20/s | 61 | 25% | 50건 | 0건 ✅ |
| 500명 vs 50좌석 | 500 | 1,000 | 100/s | 17 | 45% | 50건 | 0건 ✅ |
| **500명 vs 50좌석 (10K 유저풀)** | 500 | 1,000 | **33.4/s** | 19 | 45% | **50건** | **0건** ✅ |

> **에러율 해석**: 에러는 "좌석 이미 점유됨" 비즈니스 에러로, 동시성 제어가 정상 작동함을 의미합니다.

### 4.3 대규모 유저 풀 테스트 (10,000명)

| 항목 | 결과 |
|------|------|
| 유저 풀 | 10,000명 |
| 동시 경쟁 | 500명 |
| 경쟁 대상 | 50좌석 |
| 총 요청 | 1,000개 (토큰 500 + 좌석배정 500) |
| TPS | 33.4/s |
| 평균 응답시간 | 19ms |
| 예약 성공 | **50건** (정확히 좌석 수만큼) |
| 중복 예약 | **0건** |
| 동시성 제어 | ✅ |

**핵심 결과**: 10,000명 규모의 유저 풀에서 500명이 동시에 50좌석을 경쟁해도 중복 예약이 발생하지 않았습니다.

### 4.4 E2E 플로우 테스트 (결제 확정)

토큰 발급 → 좌석 배정 → 결제 확정까지 전체 플로우 테스트

| 단계 | 동시 사용자 | 총 요청 | TPS | 평균(ms) | 에러율 | 결제 성공 |
|------|-----------|--------|-----|---------|--------|----------|
| 2단계 | 10명 | 30 | 6.6/s | 20 | 0% | 10건 ✅ |
| 3단계 | 50명 | 150 | 30.4/s | 9 | 0% | 50건 ✅ |

**검증 결과:**
- 예약 상태: 전원 CONFIRMED
- 잔액 정합성: 좌석 등급별 차감액 정확히 일치
    - 좌석 1~10 (VIP): 110,000원 차감 → 잔액 890,000원
    - 좌석 11~30 (R석): 80,000원 차감 → 잔액 920,000원
    - 좌석 31~50 (S석): 60,000원 차감 → 잔액 940,000원
- 총 결제액: 3,900,000원 (DB 일치 ✅)
- 중복 결제: 0건 ✅

### 4.5 스트레스 테스트 (한계점 파악)

시스템이 어느 시점에 장애가 발생하는지 파악하기 위한 테스트 (1CPU / 512M, 잔액 조회 API)

| 단계 | 동시 사용자 | TPS | 평균(ms) | 최대(ms) | 에러율 | CPU | MEM | 상태 |
|------|-----------|-----|---------|---------|--------|-----|-----|------|
| 1단계 | 100명 | 2,256/s | 38 | 310 | 0% | 105% | 79% | 🟢 안정 |
| 2단계 | 200명 | **2,771/s** | 61 | 15,086 | 0% | 107% | 87% | 🟡 최대 TPS |
| 3단계 | 500명 | 2,081/s | 155 | 124,057 | 0.01% | 107% | 89% | 🟠 성능 저하 |
| 4단계 | 1,000명 | 1,799/s | 314 | 183,048 | 0.01% | 108% | 89% | 🔴 불안정 |

**핵심 발견:**
- **TPS 포화점**: ~2,800/s (200명에서 최대 도달)
- **에러 발생 시점**: 500명 이상에서 시스템 에러 발생
- **CPU 한계**: 1코어 100% 포화 상태 지속
- **응답시간 급증**: 1,000명에서 최대 응답시간 3분 초과

**스케일 아웃 기준:**
| 구간 | 동시 사용자 | 권장 |
|------|-----------|------|
| 🟢 안전 운영 | ~100명 | 단일 인스턴스 |
| 🟡 피크 허용 | ~200명 | 모니터링 강화 |
| 🔴 위험 구간 | 500명+ | **서버 증설 필요** |

---

## 5. 분석 및 결론

### 5.1 핵심 발견사항

#### 메모리 최소 요구사항
- **256MB: 서비스 불가** (OOM 발생, 에러율 100%)
- **512MB: 안정적 운영 가능**
- Spring Boot + JPA 애플리케이션 특성상 최소 512MB 이상 필요

#### CPU 영향도
- **0.5 CPU → 1 CPU**: TPS 30배 증가 (134 → 4,088)
- **1 CPU → 2 CPU**: TPS 변화 미미 (4,088 → 3,380)
- 단순 읽기 작업에서는 1 CPU로 충분

#### 메모리 영향도
- **512MB → 1GB**: 성능 차이 거의 없음
- 현재 워크로드에서 메모리는 병목이 아님

#### 동시성 제어 검증 ✅
- **Redis 분산 락 정상 작동**: 500명 동시 경쟁에서도 중복 예약 0건
- **선착순 처리 보장**: 50좌석에 정확히 50건만 예약 성공
- **데이터 정합성 100%**: 좌석별 단일 예약만 존재
- **대규모 유저 풀 검증**: 10,000명 유저 풀에서도 동일하게 동작

### 5.2 병목 분석
1. **CPU**: 0.5 CPU 이하에서 급격한 성능 저하
2. **메모리**: 256MB에서 OOM 발생
3. **분산 락**: 병목 아님 (Redis 락 획득/해제 빠름)
4. **네트워크/DB**: 현재 테스트에서는 병목 아님
5. **JMeter 클라이언트**: 10,000 스레드 시 로컬 머신 한계 도달
6. **스트레스 테스트 결과**: 동시 사용자 200명 초과 시 TPS 감소, 500명 이상에서 에러 발생


### 5.3 적정 배포 스펙 권고

| 환경 | CPU | Memory | 예상 TPS | 비고 |
|------|-----|--------|----------|------|
| **최소 스펙** | 0.5 | 512M | ~130 | 개발/테스트 환경 |
| **권장 스펙** | 1.0 | 512M | ~4,000 | 일반 운영 환경 |
| **고성능 스펙** | 2.0 | 1G | ~3,500 | 피크 트래픽 대응 |

**스트레스 테스트 기반 운영 권고:**
- 동시 사용자 100명 이하: 단일 인스턴스로 안정 운영 가능
- 동시 사용자 200명 초과 예상 시: 스케일 아웃 또는 로드밸런서 도입 권장
- TPS 2,800/s 이상 필요 시: 다중 인스턴스 구성 필수

**권장: 1 CPU / 512MB**
- 비용 대비 성능 효율 최적
- 4,000 TPS로 대부분의 트래픽 처리 가능
- 500명 동시 좌석 경쟁에서도 안정적
- 10,000명 유저 풀에서도 동시성 제어 완벽

---

## 6. 향후 계획

### 6.1 완료된 테스트
- [x] 잔액 조회 부하 테스트
- [x] 리소스별 성능 비교 (CPU/Memory)
- [x] 예약 플로우 테스트 (토큰 → 좌석 배정)
- [x] 동시성 제어 검증 (500명 경쟁)
- [x] 대규모 유저 풀 테스트 (10,000명)
- [x] E2E 플로우 테스트 (결제 확정)
- [x] 스트레스 테스트 (한계점 파악)


### 6.2 추가 테스트 필요
1. **장기 부하 테스트**: 30분 이상 지속 부하
2. **스파이크 테스트**: 순간 트래픽 급증 시나리오
3. **분산 환경 테스트**: 다중 인스턴스 환경에서의 동시성 제어

### 6.3 개선 방안
1. 결제 확정 API 응답 처리 개선 (비동기 → 동기 또는 폴링)
2. 캐시 적용 확대 (Redis)
3. 커넥션 풀 최적화
