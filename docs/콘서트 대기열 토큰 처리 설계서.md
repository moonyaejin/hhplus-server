# 콘서트 대기열 토큰 처리 설계서

> 작성자: 문예진
> 
> 작성일: 2025-11-28

---

## 1. 개요

### 1.1 배경

콘서트 티켓팅 오픈 시 수만~수십만 명이 동시에 접속합니다.
현재는 Redis 기반 대기열을 사용하고 있지만, **Kafka로 전환한다면 어떻게 설계해야 할지** 검토합니다.

### 1.2 현재 시스템 (Redis 기반)

```
[사용자] → [API Server] → [Redis]
                            ├── queue:waiting (Sorted Set) - 대기열
                            ├── queue:active (Set) - 활성 토큰
                            └── queue:token:{token} (Hash) - 토큰 정보
```

**현재 방식의 특징:**
- Sorted Set으로 FIFO 순서 보장
- `ZRANK`로 "내 순번" O(log N) 조회
- 최대 100명까지 동시 활성화

---

## 2. Kafka 기반 대기열 설계

### 2.1 기본 구조

```
[사용자] → [API Server] → [Kafka]
                            └── Topic: concert-queue (10 partitions 고정)
                                  ├── Partition 0
                                  ├── Partition 1
                                  ├── ...
                                  └── Partition 9
```

**Kafka 대기열의 핵심 원리:**
- 메시지 발행 순서 = 대기열 순서
- Partition 내에서는 순서 보장 (offset 기반)
- Consumer가 메시지를 가져가면 = 대기열 통과

---

## 3. 콘서트별 파티션 구성 전략

### 3.1 콘서트별 1:1 파티션

```yaml
Partition 0: concertId = "BTS-2025"
Partition 1: concertId = "IU-2025"
Partition 2: concertId = "BLACKPINK-2025"
```

**왜 이렇게 사용하면 안 될까?**

| 문제 | 설명 |
|------|------|
| **파티션 동적 관리 어려움** | 콘서트 추가할 때마다 파티션 추가? Kafka는 파티션 삭제 불가! |
| **비즈니스-인프라 강결합** | 콘서트 10개 → 파티션 10개, 콘서트 100개 → 파티션 100개? |
| **부하 분산 실패** | 인기 콘서트(BTS) 파티션만 과부하, 비인기 파티션은 놀고 있음 |
| **운영 복잡도** | 콘서트 오픈할 때마다 인프라팀 호출? |


### 고정 파티션 + Key 기반 해시 분산

```yaml
Topic: concert-queue
Partitions: 10개 (고정, 콘서트 개수와 무관)

# 메시지 발행 시
Key: concertId
Partition 결정: hash(concertId) % 10
```

**동작 방식:**

```
concertA (ID: "BTS-2025") 
→ hash("BTS-2025") % 10 = 3
→ Partition 3

concertB (ID: "IU-2025")
→ hash("IU-2025") % 10 = 7
→ Partition 7

concertC (ID: "BLACKPINK-2025")
→ hash("BLACKPINK-2025") % 10 = 3
→ Partition 3 (BTS와 같은 파티션!)
```

**코드 예시:**

```java
// Producer
public void enqueue(String concertId, String userId) {
    QueueMessage message = new QueueMessage(
        UUID.randomUUID().toString(),
        concertId,
        userId,
        Instant.now().toEpochMilli()
    );
    
    // concertId를 Key로 → Kafka가 hash % partitionCount 자동 계산
    kafkaTemplate.send("concert-queue", concertId, message);
}

// 결과:
// 같은 concertId → 항상 같은 파티션 → 순서 보장! 
// 파티션 수는 고정 → 콘서트 추가/삭제해도 변경 없음!
```

### 3.3 전략 비교 요약

| 항목 | 콘서트별 1:1 | 고정 + 해시 |
|------|------------|-----------|
| 파티션 수 | 콘서트 개수에 종속 | 고정 (10~30개) |
| 콘서트 추가 시 | 파티션 추가 필요 | 변경 없음 |
| 순서 보장 | 완벽 | Key 기반 보장 |
| 부하 분산 | 불균형 | 해시로 균등 분산 |
| 운영 복잡도 | 높음 | 낮음 |
---

## 4. 파티션 수 증가 시 트레이드오프

### 4.1 파티션 수와 처리량

```
파티션 1개:  [Consumer 1] ─────────────────→ 순차 처리
            처리량: 1000 TPS (Consumer 1대 한계)

파티션 10개: [Consumer 1] ──→ Partition 0, 1
            [Consumer 2] ──→ Partition 2, 3
            [Consumer 3] ──→ Partition 4, 5
            [Consumer 4] ──→ Partition 6, 7
            [Consumer 5] ──→ Partition 8, 9
            처리량: 5000 TPS (5배 증가!)
```

### 4.2 트레이드오프 정리

| 파티션 수 | 처리량 | 순서 보장 | 리소스 | Rebalancing |
|----------|----|--------|----|---------|
| 1개 | 낮음 | 전체 | 최소 | 빠름 |
| 10개 | 중간 | Key 기반️ | 중간 | 중간 |
| 50개 | 높음 | Key 기반️ | 많음 | 느림 |

### 4.3 순서 보장의 범위

**핵심 개념: 같은 Key = 같은 파티션 = 순서 보장**

```
시나리오: 10개 파티션, concertId를 Key로 사용

BTS 콘서트 예약 이벤트:
  예약1 → Key: "BTS-2025" → hash % 10 = 3 → Partition 3
  예약2 → Key: "BTS-2025" → hash % 10 = 3 → Partition 3
  예약3 → Key: "BTS-2025" → hash % 10 = 3 → Partition 3

Partition 3에서: 예약1 → 예약2 → 예약3 순서 보장!
```

**주의: 다른 Key 간에는 순서 보장 안 됨**

```
BTS 예약1 (Partition 3, offset 100)
IU 예약1  (Partition 7, offset 50)

어떤 게 먼저 처리될지? → 알 수 없음 (다른 파티션이니까)
하지만 BTS와 IU는 서로 독립적이니까 상관없음!
```

---

## 5. 비즈니스 로직과 파티션 강결합 문제

### 5.1 강결합 예시 (나쁜 예)

```java
// 비즈니스 로직이 파티션 구조에 직접 의존
public void processQueue(String concertId) {
    // 콘서트ID로 파티션 번호 직접 계산
    int partitionId = concertIdToPartitionMap.get(concertId);
    
    // 특정 파티션에서만 읽기
    consumer.assign(Collections.singletonList(
        new TopicPartition("concert-queue", partitionId)
    ));
    
    // 문제: 파티션 구조 바뀌면 이 코드도 수정해야 함
}
```

### 5.2 강결합이 발생하는 패턴들

| 패턴 | 문제점 |
|------|--------|
| `concertId → partition` 매핑 테이블 | 콘서트 추가마다 테이블 수정 |
| 파티션 수 하드코딩 | 확장 시 코드 수정 필요 |
| 특정 파티션 직접 assign | 파티션 변경 시 로직 깨짐 |
| 파티션 번호 로깅/저장 | 인프라 변경이 비즈니스에 영향 |

### 5.3 해결 방안

**방안 1: Key 기반 자동 분배**

```java
// Key만 지정, 파티션은 Kafka가 결정
kafkaTemplate.send("concert-queue", concertId, message);

// 비즈니스 로직은 파티션 구조를 전혀 모름
// Kafka가 알아서 같은 Key는 같은 파티션으로 보장
```

**방안 2: Consumer Group 활용**

```java
// Consumer Group이 파티션 자동 할당
@KafkaListener(
    topics = "concert-queue",
    groupId = "queue-processor"
)
public void process(QueueMessage message) {
    // 어느 파티션에서 왔는지 몰라도 됨
    // 비즈니스 로직만 집중
    activateUser(message);
}
```

**방안 3: 포트/어댑터 패턴으로 추상화**

```java
// 인터페이스
public interface QueuePort {
    void enqueue(String concertId, String userId);
    QueueToken dequeue(String concertId);
    Long getWaitingPosition(String token);
}

// Kafka 구현체
@Component
public class KafkaQueueAdapter implements QueuePort {
    // 파티션 관련 로직은 여기에만!
    // 비즈니스는 QueuePort 인터페이스만 의존
}
```

---

## 6. Kafka 대기열의 한계점

### 6.1 "내 순번" 조회 문제

**Redis:**
```java
// O(log N) - 매우 빠름
Long rank = redisTemplate.opsForZSet().rank(WAITING_QUEUE, token);
```

**Kafka:**
```java
// 직접 조회 불가능
// Kafka는 "몇 번째 메시지인지" 조회하는 API가 없음

// 해결책 1: Redis에 순번만 따로 저장
public Long getWaitingPosition(String token) {
    return redisTemplate.opsForValue().get("queue:waiting:" + token);
}

// 해결책 2: DB에 저장
// → 결국 Kafka + Redis/DB 혼합 사용 필요할 듯
```

### 6.2 대기열 이탈 처리

**Redis:**
```java
redisTemplate.opsForZSet().remove(WAITING_QUEUE, token);
```

**Kafka:**
```java
// 메시지 삭제 불가능!
// 해결책: "취소" 메시지를 발행하고 Consumer에서 무시

// 1. 취소 토픽에 발행
kafkaTemplate.send("concert-queue-cancel", token, cancelMessage);

// 2. Consumer에서 처리 시 취소 여부 확인
@KafkaListener(topics = "concert-queue")
public void process(QueueMessage message) {
    if (isCancelled(message.tokenId())) {
        return;  // 무시
    }
    // 정상 처리
}
```

---

## 7. 최종 결론: 대기열에는 Redis가 더 적합한 것 같다!

### 7.1 대기열 시스템 구조

```
대기열 = Redis 유지
  - 순번 조회 필요 (ZRANK)
  - 실시간 이탈/복귀 필요
  - 이미 잘 동작하고 있음

이벤트 전송 = Kafka 적용
  - 데이터 플랫폼 전송
  - 결제 비동기화
  - 장애 격리, 재처리 필요
```
