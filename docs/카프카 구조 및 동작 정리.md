# Kafka 구조 및 동작 정리

**작성자**: 문예진

**작성일**: 2025-11-24

**프로젝트**: 콘서트 예약 시스템

---

## 0. Kafka 한줄 정리

```
실시간으로 많은 데이터를 빠르게 처리하기 위한 분산 메시지 시스템
파티션 구조로 병렬 처리가 가능하고, 파티션 안에서는 순서를 보장함
메시지를 디스크에 저장해서 장애가 나도 재처리할 수 있음
```

---

## 1. Kafka를 왜 쓰는지

### Spring Event의 한계

프로젝트에서 Spring Event를 썼는데, 이런 한계가 있었다:

- 메모리에만 있어서 처리 실패하면 이벤트가 사라짐
- Consumer 서비스에 장애가 나면 재전송 로직을 직접 구현해야 함
- 새로운 Consumer 추가할 때마다 코드 수정 필요

### Kafka로 해결되는 부분

- 메시지를 디스크에 저장 → 기본 7일간 보관, 재처리 가능
- Offset이라는 개념으로 "어디까지 읽었는지" 기억
- Consumer Group으로 여러 서비스가 독립적으로 소비 가능

---

## 2. Kafka 구성요소

### Producer

Kafka에 메시지를 발행하는 쪽. 내 프로젝트로 치면 ReservationService가 Producer다.

```java
kafkaTemplate.send(
    "reservation-confirmed",              // Topic
    event.getConcertScheduleId().toString(), // Key
event                                   // Value
);
```

### Broker

Kafka 서버. 메시지를 디스크에 저장하고 관리한다.

- 클러스터로 구성하면 고가용성 확보
- Replication으로 데이터 복제

### Topic

메시지를 종류별로 분류하는 단위. DB의 테이블 같은 개념이라고 이해했다.

```
reservation-confirmed  (예약 확정 이벤트)
payment-completed      (결제 완료 이벤트)
notification-send      (알림 발송 이벤트)
```

### Partition

**Topic을 여러 개로 나눈 물리적 단위 - Kafka의 핵심이다.**

```
Topic: reservation-confirmed (10개 파티션)

Partition 0: [이벤트1] [이벤트2] [이벤트3] ...
Partition 1: [이벤트4] [이벤트5] [이벤트6] ...
Partition 2: [이벤트7] [이벤트8] [이벤트9] ...
```

**내가 이해한 Partition의 특징:**

- **순서 보장**: 같은 파티션 안에서만 순서가 유지됨
- **병렬 처리**: 파티션 개수만큼 Consumer를 늘려서 병렬 처리 가능
- **Key 기반 분배**: Key 해시값으로 어느 파티션에 들어갈지 결정 (`hash(key) % partition_count`)

**Offset:**

- 파티션 내 메시지의 위치 번호 (0부터 시작)
- Consumer가 "여기까지 읽었다" 체크포인트
- 장애 시 마지막 Offset부터 재시작

### Consumer

메시지를 가져와서 처리하는 쪽. Pull 방식으로 동작한다.

```java
@KafkaListener(topics = "reservation-confirmed", groupId = "ranking-service")
public void consume(ConsumerRecord<String, Event> record, Acknowledgment ack) {
    rankingService.update(record.value());
    ack.acknowledge();  // Offset Commit
}
```

### Consumer Group

같은 Topic을 여러 Consumer가 협력해서 소비하는 단위

```
Topic: reservation-confirmed

[Consumer Group: ranking-service]
- Consumer 1 → Partition 0, 1
- Consumer 2 → Partition 2, 3

[Consumer Group: notification-service]
- Consumer 1 → Partition 0, 1
- Consumer 2 → Partition 2, 3

→ 각 그룹은 독립적으로 Offset 관리!
```

**핵심 규칙:**

- 한 파티션은 같은 그룹 내에서 하나의 Consumer만 담당
- 각 그룹은 독립적으로 Offset 관리
- Consumer가 죽으면 Rebalancing 발생 (파티션 재분배)

---

## 3. 데이터가 흐르는 과정

```
[ReservationService]
    ↓ (1) 이벤트 발행
    │ Key: "concert-schedule-123"
    ↓
[Kafka Broker]
    ↓ (2) 파티션 결정
    │ hash("concert-schedule-123") % 10 = 3
    ↓
[Partition 3에 저장]
    │ Offset: 1002
    ↓
    ├──────────┬──────────┐
    ↓          ↓          ↓
[Ranking]  [Notification]  [Data]
Offset:1000    Offset:5      Offset:0
    ↓          ↓          ↓
처리 완료     처리 완료     처리 완료
    ↓          ↓          ↓
Commit 1001   Commit 6    Commit 1
```

### 단계별 정리

**Step 1: Producer → Broker**

1. 트랜잭션 커밋 후 이벤트 발행
2. Key 해시값으로 파티션 결정
3. 해당 파티션 디스크에 저장

**Step 2: Broker 저장 형태**

```
Partition 3:
Offset:1000 | Key: schedule-123 | Value: Event{...}
Offset:1001 | Key: schedule-123 | Value: Event{...}
Offset:1002 | Key: schedule-123 | Value: Event{...} ← 새로 저장
```

**Step 3: Consumer Poll**

1. Consumer가 Broker에 메시지 요청 (Pull)
2. Broker가 현재 Offset부터 메시지 전달
3. Consumer가 비즈니스 로직 처리
4. 처리 성공 후 Offset Commit

**Step 4: 장애 대응**

```
상황: Consumer가 Offset 1002 처리 중 장애 발생
Last Commit: Offset 1001

재시작 →
Offset 1002부터 다시 처리
→ 메시지 유실 없음 (At-Least-Once)
```

---

## 4. Partition 전략 (공부하면서 정리한 내용)

처음엔 "콘서트별로 파티션을 나누면 되지 않나?" 생각했는데... 더 생각해보니 이렇게 하지 않을 것 같아서 이유를 정리해봤다.

**이유:**
- 콘서트 추가/종료마다 파티션 변경이 어려움
- 비즈니스와 인프라가 강하게 결합됨
- 인기 콘서트와 비인기 콘서트 간 부하 불균형

### 예상

```java
// 파티션 10개로 고정
// 같은 concertScheduleId → 항상 같은 파티션
// 10개 파티션 → 최대 10개 Consumer 병렬 처리
```

### Key 선택 기준

공부하면서 이해한 건, "무엇의 순서를 보장해야 하는가"가 Key를 정하는 기준이라는 것이다.

내 프로젝트에 적용한다면 이렇게 설계할 것 같다:

| 이벤트 유형 | Key | 이유 |
|------------|-----|------|
| **예약 확정/취소** | `scheduleId` | 같은 콘서트 스케줄의 랭킹 업데이트는 순서대로 처리되어야 함<br/>(+1, +1, -1 순서가 바뀌면 랭킹 오류) |
| **지갑 충전/사용** | `userId` | 같은 사용자의 잔액 변경은 순서 보장 필요<br/>(충전 → 사용 → 환불 순서 중요) |
| **데이터 플랫폼 전송** | `reservationId` | 같은 예약의 상태 변화를 추적하기 위해<br/>(임시 배정 → 확정 → 취소) |

**실제 운영을 상상해보면서 이런 시나리오도 고려해봤다:**

| 확장 시나리오 | Key | 이유 |
|------------|-----|------|
| **대기열 활성화** | `scheduleId` | 콘서트별로 대기열 토큰 순차 처리 |
| **알림 발송** | `userId` | 같은 유저의 알림은 순서대로 전송 |

---

## 5. Kafka 장단점

### 장점

| 장점 | 내가 이해한 내용 |
|------|------|
| **높은 처리량** | 파티션 단위로 병렬 처리, 초당 수백만 건 가능 |
| **영속성** | 디스크 저장, 기본 7일 보관 |
| **재처리 가능** | Offset 리셋으로 과거 데이터 다시 처리 |
| **확장성** | Broker/Partition/Consumer 수평 확장 가능 |
| **서비스 독립성** | Consumer Group으로 각자 처리 |

### 단점

| 단점 | 내가 느낀 점 |
|------|------|
| **운영 복잡도** | Broker 설정, 모니터링 필요 |
| **정확히 한 번 어려움** | At-Least-Once가 기본, 중복 처리 가능 |
| **순서 보장 제한** | 파티션 내에서만 보장됨 |
| **학습 곡선** | 개념이 많아서 처음엔 어려웠음 |

### Spring Event와 비교

| 항목 | Spring Event | Kafka |
|------|-----------|-----|
| 저장 위치 | 메모리 | 디스크 |
| 재처리 | 불가능 | 가능 |
| 확장성 | 제한적 | 뛰어남 |
| 복잡도 | 낮음 | 높음 |
| 적합한 경우 | 소규모 | 대규모 |

---

## 핵심 정리
공부하면서 이해한 Kafka의 핵심:

1. **Kafka = 영속적 이벤트 저장소**
    - 메모리가 아닌 디스크에 저장
    - 일정 기간 보관 후 자동 삭제

2. **Partition = 순서와 병렬의 균형**
    - 같은 Key → 같은 파티션 → 순서 보장
    - 파티션 수 = 최대 병렬 Consumer 수

3. **Consumer Group = 독립적 구독**
    - 같은 이벤트를 여러 시스템이 각자 처리
    - 서비스 간 독립성 확보

4. **실무 전략 = 고정 파티션 + Key 라우팅**
    - 파티션 수는 고정
    - Key로 메시지 분배
    - 비즈니스와 인프라 분리